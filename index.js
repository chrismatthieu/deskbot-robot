require('dotenv').config();
const { Cam } = require('onvif');
const ollama = require('ollama').default;
const OpenAI = require('openai');
const sharp = require('sharp');
const fs = require('fs');
const path = require('path');
const { spawn } = require('child_process');
const record = require('node-record-lpcm16');

// Camera configuration
const cameraConfig = {
  hostname: process.env.CAMERA_HOSTNAME || '192.168.0.42',
  username: process.env.CAMERA_USERNAME || 'admin',
  password: process.env.CAMERA_PASSWORD || 'admin123',
  port: parseInt(process.env.CAMERA_PORT) || 80
};

// AI Provider Configuration
const AI_PROVIDER = process.env.AI_PROVIDER || 'ollama'; // 'ollama' or 'chatgpt'

// AI Vision configuration
const AI_CONFIG = {
  model: 'qwen2.5vl:3b',
  analysisInterval: 1000, // Analyze every 5 seconds
  confidenceThreshold: 0.7,
  maxRetries: 3
};

// Voice processing configuration
const VOICE_CONFIG = {
  sampleRate: 16000,
  channels: 1,
  threshold: 0.1, // Audio level threshold for voice detection
  silenceTimeout: 2000, // Stop recording after 2 seconds of silence
  recordingDuration: 5000 // Maximum recording duration in ms
};

// Wake word detection configuration
const WAKE_WORD_CONFIG = {
  wakePhrase: 'jarvis',
  sampleRate: 16000,
  channels: 1,
  threshold: 0.05, // Lower threshold for wake word detection
  silenceTimeout: 1000, // Shorter silence timeout for wake detection
  recordingDuration: 3000, // Shorter recording for wake word
  enabled: true // Enable/disable wake word detection
};

let rtspStream = null;
let visionAnalysisActive = false;
let lastAnalysisTime = 0;
let aiAnalysisInProgress = false;
let gestureInProgress = false;
let voiceRecordingActive = false;
let currentAudioStream = null;

console.log('ðŸ” Connecting to Amcrest camera...');
console.log(`ðŸ“ IP: ${cameraConfig.hostname}`);
console.log(`ðŸ‘¤ Username: ${cameraConfig.username}`);
console.log(`ðŸ¤– AI Provider: ${AI_PROVIDER.toUpperCase()}`);

// Initialize AI clients
let openai = null;
if (AI_PROVIDER === 'chatgpt') {
  openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY
  });
  console.log('ðŸ¤– Initializing AI Vision with ChatGPT...');
} else {
  console.log('ðŸ¤– Initializing AI Vision with Ollama...');
}

// Initialize AI connection
async function initializeAI() {
  if (AI_PROVIDER === 'chatgpt') {
    try {
      console.log('ðŸ§  Checking ChatGPT connection...');
      // Test the connection with a simple request
      const response = await openai.chat.completions.create({
        model: 'gpt-4o-mini',
        messages: [{ role: 'user', content: 'Hello' }],
        max_tokens: 5
      });
      console.log('âœ… ChatGPT connection successful!');
      return true;
    } catch (error) {
      console.error('âŒ Failed to connect to ChatGPT:', error.message);
      console.log('ðŸ’¡ Make sure your OpenAI API key is valid');
      return false;
    }
  } else {
    try {
      console.log('ðŸ§  Checking Ollama connection...');
      const models = await ollama.list();
      console.log('ðŸ“‹ Available models:', models.models.map(m => m.name));
      
      // Check if qwen2.5vl:3b is available
      const hasQwenModel = models.models.some(m => m.name.includes('qwen2.5vl:3b'));
      if (!hasQwenModel) {
        console.log('âš ï¸  qwen2.5vl:3b model not found. Please install it with: ollama pull qwen2.5vl:3b');
        console.log('ðŸ”„ Falling back to llama3.2-vision model...');
        AI_CONFIG.model = 'llama3.2-vision';
      } else {
        console.log('âœ… qwen2.5vl:3b model found!');
      }
      
      return true;
    } catch (error) {
      console.error('âŒ Failed to connect to Ollama:', error.message);
      console.log('ðŸ’¡ Make sure Ollama is running: ollama serve');
      return false;
    }
  }
}

// Capture frame from RTSP stream and convert to base64
async function captureFrame(rtspUrl) {
  return new Promise((resolve, reject) => {
    try {
      const outputPath = './temp_frame.jpg';
      
      // Use ffmpeg to capture a single frame from RTSP stream
      const ffmpegArgs = [
        '-i', rtspUrl,
        '-vframes', '1',
        '-f', 'image2',
        '-y', // Overwrite output file
        outputPath
      ];
      
      console.log('      ðŸ“¹ Capturing frame with ffmpeg...');
      
      const ffmpeg = spawn('ffmpeg', ffmpegArgs, {
        stdio: ['ignore', 'pipe', 'pipe']
      });
      
      let stderr = '';
      
      ffmpeg.stderr.on('data', (data) => {
        stderr += data.toString();
      });
      
      ffmpeg.on('close', async (code) => {
        if (code === 0 && fs.existsSync(outputPath)) {
          try {
            // Use sharp to process the image and convert to base64
            const imageBuffer = await sharp(outputPath)
              .resize(640, 480, { fit: 'inside' }) // Resize for better performance
              .jpeg({ quality: 80 })
              .toBuffer();
            
            const base64String = imageBuffer.toString('base64');
            
            // Clean up
            fs.unlinkSync(outputPath);
            
            console.log('      âœ… Frame captured and processed successfully');
            resolve(base64String);
          } catch (error) {
            console.error('      âŒ Error processing image:', error.message);
            reject(error);
          }
        } else {
          console.error('      âŒ FFmpeg failed:', stderr);
          reject(new Error(`FFmpeg failed with code ${code}: ${stderr}`));
        }
      });
      
      ffmpeg.on('error', (error) => {
        console.error('      âŒ FFmpeg error:', error.message);
        reject(error);
      });
      
    } catch (error) {
      reject(error);
    }
  });
}

// Unified AI chat function that works with both Ollama and ChatGPT
async function aiChat(messages) {
  try {
    if (AI_PROVIDER === 'chatgpt') {
      console.log('ðŸ¤– Using ChatGPT for AI analysis...');
      
      // Convert messages for ChatGPT format
      const chatGptMessages = messages.map(msg => {
        if (msg.images && msg.images.length > 0) {
          // Convert base64 to proper format for ChatGPT
          return {
            role: msg.role,
            content: [
              {
                type: 'text',
                text: msg.content
              },
              {
                type: 'image_url',
                image_url: {
                  url: `data:image/jpeg;base64,${msg.images[0]}`
                }
              }
            ]
          };
        } else {
          return {
            role: msg.role,
            content: msg.content
          };
        }
      });
      
      console.log('ðŸ“¤ Sending to ChatGPT:', {
        model: 'gpt-4o',
        messageCount: chatGptMessages.length,
        hasImage: chatGptMessages.some(msg => msg.content && Array.isArray(msg.content) && msg.content.some(c => c.type === 'image_url'))
      });
      
      const response = await openai.chat.completions.create({
        model: 'gpt-4o', // Use vision-capable model
        messages: chatGptMessages,
        max_tokens: 50,
        temperature: 0.1
      });
      
      console.log('ðŸ“¥ ChatGPT response:', response.choices[0].message.content);
      return response.choices[0].message.content;
    } else {
      console.log('ðŸ¤– Using Ollama for AI analysis...');
      const response = await ollama.chat({
        model: AI_CONFIG.model,
        messages: messages,
        stream: false
      });
      return response.message.content;
    }
  } catch (error) {
    console.error('âŒ AI chat failed:', error.message);
    return null;
  }
}

// Analyze image with AI Vision model
async function analyzeImage(imageBase64) {
  try {
    console.log('ðŸ” Analyzing image with AI...');
    
    const messages = [
      {
        role: 'system',
        content: 'You are a helpful AI assistant that analyzes security camera footage. Describe what you see in a clear, concise manner using no more than 6 words. Focus on people, objects, activities, and any potential security concerns. Keep descriptions brief but informative.'
      },
      {
        role: 'user',
        content: 'What do you see in this security camera image?',
        images: [imageBase64]
      }
    ];
    
    return await aiChat(messages);
  } catch (error) {
    console.error('âŒ AI analysis failed:', error.message);
    return null;
  }
}

// Start AI vision analysis (disabled - only for voice questions)
async function startVisionAnalysis(rtspUrl) {
  console.log('ðŸ‘ï¸  AI Vision Analysis ready (only for voice questions)...');
  console.log('ðŸ“Š No continuous analysis - only responds to voice questions');
  console.log('ðŸŽ¤ Press "V" to ask a question and get AI response with gestures!');
}

// Announce vision results (no automatic gestures)
function announceVisionResult(analysis) {
  console.log('ðŸ“¢ Vision Announcement:', analysis);
  
  // Here you could integrate with text-to-speech
  // For now, we'll just log it
  const timestamp = new Date().toLocaleTimeString();
  console.log(`ðŸ• [${timestamp}] Camera sees: ${analysis}`);
  
  // No automatic gestures - only respond to user questions
  console.log('ðŸ¤– Camera is observing... Press "V" to ask a question!');
}

// Start voice recording for user questions
async function startVoiceRecording() {
  if (voiceRecordingActive) {
    console.log('ðŸŽ¤ Voice recording already active');
    return;
  }
  
  voiceRecordingActive = true;
  console.log('ðŸŽ¤ Starting voice recording... Speak your question now!');
  console.log('ðŸŽ¤ Speak clearly into the camera microphone...');
  
  try {
    // Use camera microphone recording (which we know works well)
    const audioFile = await recordAudioFromCamera();
    if (audioFile) {
      await processVoiceInput(null, audioFile);
    } else {
      console.log('âŒ No audio recorded from camera microphone');
    }
  } catch (error) {
    console.log('âŒ Camera microphone recording failed:', error.message);
  } finally {
    // Always reset the flag when done
    voiceRecordingActive = false;
    console.log('ðŸŽ¤ Voice recording session ended');
  }
}

// Start local microphone recording
function startLocalRecording() {
  console.log('ðŸŽ¤ Starting local microphone recording...');
  console.log('ðŸŽ¤ Recording config:', {
    sampleRate: VOICE_CONFIG.sampleRate,
    threshold: VOICE_CONFIG.threshold,
    silence: VOICE_CONFIG.silenceTimeout,
    duration: VOICE_CONFIG.recordingDuration
  });
  
  const recording = record.record({
    sampleRateHertz: VOICE_CONFIG.sampleRate,
    threshold: VOICE_CONFIG.threshold,
    silence: VOICE_CONFIG.silenceTimeout,
    recordProgram: 'rec' // Use 'rec' command for recording
  });
  
  const audioChunks = [];
  
  recording.stream()
    .on('data', (chunk) => {
      audioChunks.push(chunk);
      console.log('ðŸŽ¤ Audio chunk received:', chunk.length, 'bytes');
    })
    .on('end', async () => {
      console.log('ðŸŽ¤ Local voice recording completed');
      console.log('ðŸŽ¤ Total audio chunks:', audioChunks.length);
      voiceRecordingActive = false;
      
      if (audioChunks.length > 0) {
        const audioBuffer = Buffer.concat(audioChunks);
        console.log('ðŸŽ¤ Total audio buffer size:', audioBuffer.length, 'bytes');
        await processVoiceInput(audioBuffer);
      } else {
        console.log('âš ï¸  No audio data recorded');
      }
    })
    .on('error', (error) => {
      console.error('âŒ Local voice recording error:', error.message);
      voiceRecordingActive = false;
    });
  
  currentAudioStream = recording;
  
  // Stop recording after maximum duration
  setTimeout(() => {
    if (voiceRecordingActive) {
      console.log('â° Voice recording timeout reached');
      recording.stop();
    }
  }, VOICE_CONFIG.recordingDuration);
}

// Process voice input and convert to text
async function processVoiceInput(audioBuffer, audioFile = null) {
  try {
    console.log('ðŸ”Š Processing voice input...');
    
    let tempAudioFile = audioFile;
    
    if (audioBuffer && !audioFile) {
      // Save audio buffer to temporary file
      tempAudioFile = './temp_voice.wav';
      fs.writeFileSync(tempAudioFile, audioBuffer);
    }
    
    // Convert speech to text using a simple approach
    // For now, we'll use a placeholder - you can integrate with Whisper API or similar
    const userQuestion = await convertSpeechToText(tempAudioFile);
    
    if (userQuestion) {
      console.log('ðŸŽ¤ User question:', userQuestion);
      await handleUserQuestion(userQuestion);
    }
    
    // Clean up
    if (tempAudioFile && fs.existsSync(tempAudioFile)) {
      fs.unlinkSync(tempAudioFile);
    }
    
  } catch (error) {
    console.error('âŒ Voice processing error:', error.message);
  }
}

// Convert speech to text using real speech recognition
async function convertSpeechToText(audioFile) {
  console.log('ðŸ”¤ *** ENTERING convertSpeechToText function ***');
  console.log('ðŸ”¤ Converting speech to text...');
        console.log('      ðŸ“ Input audio file:', audioFile);
      console.log('      ðŸ“ Input file exists:', fs.existsSync(audioFile));
      console.log('      ðŸ“ Input file size:', fs.existsSync(audioFile) ? fs.statSync(audioFile).size : 'N/A', 'bytes');
      
      try {
        // Check if the input file is already WAV format
        const isWavFile = audioFile.toLowerCase().endsWith('.wav');
        let wavFile = audioFile;
        
        if (!isWavFile) {
          // Convert AAC to WAV format for better compatibility
          wavFile = audioFile.replace('.g711a', '.wav');
          console.log('      ðŸ”„ Converting AAC to WAV:', wavFile);
      
          // Use ffmpeg to convert AAC to WAV
          const { spawn } = require('child_process');
          console.log('      ðŸ”§ FFmpeg command:', 'ffmpeg', '-f', 'aac', '-i', audioFile, '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', '-y', wavFile);
          
          const ffmpeg = spawn('ffmpeg', [
            '-f', 'aac',  // Force AAC format
            '-i', audioFile,
            '-acodec', 'pcm_s16le',
            '-ar', '16000',
            '-ac', '1',
            '-y', // Overwrite
            wavFile
          ]);
        
        let stderr = '';
        let stdout = '';
        ffmpeg.stderr.on('data', (data) => {
          stderr += data.toString();
        });
        ffmpeg.stdout.on('data', (data) => {
          stdout += data.toString();
        });
    
            await new Promise((resolve, reject) => {
          ffmpeg.on('close', (code) => {
            console.log('      ðŸ”§ FFmpeg exit code:', code);
            console.log('      ðŸ”§ FFmpeg stdout:', stdout);
            console.log('      ðŸ”§ FFmpeg stderr:', stderr);
            if (code === 0) {
              console.log('      âœ… Audio converted to WAV format');
              // Add a small delay to ensure file system sync
              setTimeout(() => {
                console.log('      â±ï¸  Waiting for file system sync...');
                resolve();
              }, 100);
            } else {
              console.log('      âŒ FFmpeg conversion failed with code', code);
              reject(new Error(`FFmpeg conversion failed with code ${code}`));
            }
          });
      
      ffmpeg.on('error', (error) => {
        console.log('      âŒ FFmpeg error:', error.message);
        reject(error);
      });
    });
        } else {
          console.log('      âœ… Input file is already WAV format, no conversion needed');
        }
    
        // Try to use whisper-node for speech recognition
        try {
          // Capture working directory BEFORE loading whisper-node (which changes it)
          const originalCwd = process.cwd();
          console.log('      ðŸ“ Original working directory:', originalCwd);
          
          console.log('      ðŸ§  Loading Whisper model...');
          const { whisper } = require('whisper-node');
          
          console.log('      ðŸŽ¤ Starting speech recognition...');
          
          // Use absolute path to ensure we're looking in the right directory
          const absoluteWavPath = require('path').resolve(originalCwd, wavFile);
          console.log('      ðŸ“ Absolute WAV file path:', absoluteWavPath);
          console.log('      ðŸ“ WAV file exists:', fs.existsSync(absoluteWavPath));
          if (fs.existsSync(absoluteWavPath)) {
            console.log('      ðŸ“ WAV file size:', fs.statSync(absoluteWavPath).size, 'bytes');
          }
      
                // Add timeout to prevent hanging
          const whisperPromise = whisper(absoluteWavPath, {
            language: 'en',
            modelName: 'tiny' // Use the tiny model that we know works
          });
      
      const timeoutPromise = new Promise((_, reject) => {
        setTimeout(() => reject(new Error('Whisper timeout after 10 seconds')), 10000);
      });
      
      const result = await Promise.race([whisperPromise, timeoutPromise]);
      
            console.log('      ðŸ“Š Whisper result:', result);
      console.log('      ðŸ“Š Whisper result type:', typeof result);
      
      // Clean up the temporary WAV file
      if (fs.existsSync(absoluteWavPath)) {
        fs.unlinkSync(absoluteWavPath);
        console.log('      ðŸ§¹ Cleaned up WAV file');
      }
      
      // Extract text from Whisper result (it returns an array of segments)
      let transcribedText = '';
      if (result && Array.isArray(result)) {
        transcribedText = result.map(segment => segment.speech).join(' ').trim();
        console.log('      ðŸ“Š Extracted text from segments:', transcribedText);
      } else if (result && result.text) {
        transcribedText = result.text;
        console.log('      ðŸ“Š Using result.text:', transcribedText);
      }
      
      if (transcribedText && transcribedText.length > 0) {
        console.log('ðŸŽ¤ Transcribed text:', transcribedText);
        return transcribedText;
      } else {
        console.log('âš ï¸  No speech detected or recognition failed');
        return 'No speech detected';
      }
      
    } catch (whisperError) {
      console.log('âŒ Whisper error:', whisperError.message);
      console.log('      ðŸ” Whisper error details:', whisperError);
      
      // Fallback: analyze audio characteristics
      try {
        console.log('      ðŸ“Š Falling back to audio analysis...');
        const { spawn } = require('child_process');
        
        // Use ffmpeg to analyze audio characteristics
        const analyze = spawn('ffmpeg', [
          '-i', wavFile,
          '-af', 'volumedetect',
          '-f', 'null',
          '-'
        ], { stdio: 'pipe' });
        
        let stderr = '';
        analyze.stderr.on('data', (data) => {
          stderr += data.toString();
        });
        
        await new Promise((resolve, reject) => {
          analyze.on('close', (code) => {
            if (code === 0 || code === 1) { // ffmpeg returns 1 for analysis
              console.log('      ðŸ“Š Audio analysis completed');
              resolve();
            } else {
              reject(new Error(`Audio analysis failed`));
            }
          });
        });
        
        // Clean up the temporary WAV file
        if (fs.existsSync(wavFile)) {
          fs.unlinkSync(wavFile);
        }
        
        // Check if there's significant audio content
        if (stderr.includes('mean_volume') && !stderr.includes('mean_volume: -inf dB')) {
          console.log('ðŸŽ¤ Audio detected with speech content');
          return 'Speech detected but transcription unavailable';
        } else {
          console.log('âš ï¸  No significant audio content detected');
          return 'No speech detected';
        }
        
      } catch (fallbackError) {
        console.log('âŒ Fallback analysis also failed:', fallbackError.message);
        return 'Audio recorded but could not be transcribed';
      }
    }
    
  } catch (error) {
    console.log('âŒ Speech recognition error:', error.message);
    console.log('      ðŸ” Error details:', error);
    return 'Audio recorded but speech recognition failed';
  }
}

// Alternative: Capture audio from RTSP stream
async function captureAudioFromRTSP(rtspUrl) {
  return new Promise((resolve, reject) => {
    try {
      const outputPath = './temp_audio.wav';
      
      // Use ffmpeg to extract audio from RTSP stream
      const ffmpegArgs = [
        '-i', rtspUrl,
        '-vn', // No video
        '-acodec', 'pcm_s16le', // Audio codec
        '-ar', '16000', // Sample rate
        '-ac', '1', // Mono
        '-t', '5', // 5 seconds duration
        '-y', // Overwrite
        outputPath
      ];
      
      console.log('      ðŸŽµ Capturing audio from RTSP...');
      
      const ffmpeg = spawn('ffmpeg', ffmpegArgs, {
        stdio: ['ignore', 'pipe', 'pipe']
      });
      
      let stderr = '';
      
      ffmpeg.stderr.on('data', (data) => {
        stderr += data.toString();
      });
      
      ffmpeg.on('close', (code) => {
        if (code === 0 && fs.existsSync(outputPath)) {
          console.log('      âœ… Audio captured successfully');
          resolve(outputPath);
        } else {
          console.error('      âŒ Audio capture failed:', stderr);
          reject(new Error(`Audio capture failed with code ${code}`));
        }
      });
      
      ffmpeg.on('error', (error) => {
        console.error('      âŒ Audio capture error:', error.message);
        reject(error);
      });
      
    } catch (error) {
      reject(error);
    }
  });
}

// Test microphone and speaker with record and playback using Amcrest API
async function testMicrophoneAndSpeaker() {
  console.log('\nðŸŽ¤ Testing microphone and speaker using Amcrest API...');
  
  try {
    // Step 1: Record audio from camera microphone using getAudio
    console.log('ðŸ“¹ Recording 5 seconds of audio from camera microphone...');
    console.log('ðŸŽ¤ Speak something into the camera microphone now!');
    
    const audioFile = await recordAudioFromCamera();
    
    if (audioFile && fs.existsSync(audioFile)) {
      console.log('âœ… Audio recording completed!');
      console.log(`ðŸ“ Audio file saved: ${audioFile} (${fs.statSync(audioFile).size} bytes)`);
      
      // Step 2: Play back the recorded audio through camera speaker
      console.log('ðŸ”Š Playing back recorded audio through camera speaker...');
      console.log('ðŸ”§ DEBUG: About to call playAudioThroughCamera function');
      await playAudioThroughCamera(audioFile);
      console.log('ðŸ”§ DEBUG: playAudioThroughCamera completed');
      
      // Step 3: Try to convert speech to text
      console.log('ðŸŽ¤ Attempting speech-to-text conversion...');
      console.log('ðŸ”§ DEBUG: About to call convertSpeechToText function');
      try {
        const transcribedText = await convertSpeechToText(audioFile);
        console.log('ðŸ“ Transcribed text:', transcribedText);
        console.log('ðŸ”§ DEBUG: Speech-to-text completed successfully');
      } catch (sttError) {
        console.log('âš ï¸  Speech-to-text failed:', sttError.message);
        console.log('ðŸ”§ DEBUG: Speech-to-text error details:', sttError);
      }
      
      // Step 4: Clean up
      if (fs.existsSync(audioFile)) {
        fs.unlinkSync(audioFile);
        console.log('ðŸ§¹ Cleaned up temporary audio file');
      }
    } else {
      console.log('âš ï¸  No audio file was created or file is empty');
    }
    
  } catch (error) {
    console.error('âŒ Microphone/speaker test failed:', error.message);
  }
}

// Record audio from camera using Amcrest getAudio API
async function recordAudioFromCamera() {
  return new Promise((resolve, reject) => {
    try {
      const http = require('http');
      const crypto = require('crypto');
      const outputPath = './temp_audio.g711a';
      
      // First request to get digest challenge
      const initialOptions = {
        hostname: cameraConfig.hostname,
        port: cameraConfig.port,
        path: '/cgi-bin/audio.cgi?action=getAudio&httptype=singlepart&channel=1',
        method: 'GET',
        headers: {
          'User-Agent': 'Amcrest-Camera-Client/1.0'
        }
      };
      
      console.log('      ðŸŽµ Recording audio from camera microphone...');
      console.log(`      ðŸ” Auth: ${cameraConfig.username}:${cameraConfig.password}`);
      console.log(`      ðŸŒ URL: http://${cameraConfig.hostname}:${cameraConfig.port}${initialOptions.path}`);
      
      const initialReq = http.request(initialOptions, (res) => {
        console.log(`      ðŸ“¡ Initial HTTP Response: ${res.statusCode}`);
        
        if (res.statusCode === 401 && res.headers['www-authenticate']) {
          // Parse digest challenge
          const authHeader = res.headers['www-authenticate'];
          console.log(`      ðŸ” Digest challenge: ${authHeader}`);
          
          // Extract digest parameters
          const realm = authHeader.match(/realm="([^"]+)"/)?.[1];
          const nonce = authHeader.match(/nonce="([^"]+)"/)?.[1];
          const qop = authHeader.match(/qop="([^"]+)"/)?.[1];
          const opaque = authHeader.match(/opaque="([^"]+)"/)?.[1];
          
          if (realm && nonce) {
            // Generate digest response
            const cnonce = crypto.randomBytes(16).toString('hex');
            const nc = '00000001';
            const uri = initialOptions.path;
            const method = 'GET';
            
            // Calculate digest response
            const ha1 = crypto.createHash('md5').update(`${cameraConfig.username}:${realm}:${cameraConfig.password}`).digest('hex');
            const ha2 = crypto.createHash('md5').update(`${method}:${uri}`).digest('hex');
            
            let response;
            if (qop === 'auth') {
              response = crypto.createHash('md5').update(`${ha1}:${nonce}:${nc}:${cnonce}:${qop}:${ha2}`).digest('hex');
            } else {
              response = crypto.createHash('md5').update(`${ha1}:${nonce}:${ha2}`).digest('hex');
            }
            
            // Build digest authorization header
            let digestAuth = `Digest username="${cameraConfig.username}", realm="${realm}", nonce="${nonce}", uri="${uri}", response="${response}"`;
            if (opaque) digestAuth += `, opaque="${opaque}"`;
            if (qop) digestAuth += `, qop=${qop}, nc=${nc}, cnonce="${cnonce}"`;
            
            console.log(`      ðŸ” Digest auth: ${digestAuth}`);
            
            // Make authenticated request
            const authOptions = {
              hostname: cameraConfig.hostname,
              port: cameraConfig.port,
              path: '/cgi-bin/audio.cgi?action=getAudio&httptype=singlepart&channel=1',
              method: 'GET',
              headers: {
                'Authorization': digestAuth,
                'User-Agent': 'Amcrest-Camera-Client/1.0'
              }
            };
            
            const authReq = http.request(authOptions, (authRes) => {
              console.log(`      ðŸ“¡ Authenticated HTTP Response: ${authRes.statusCode}`);
              console.log(`      ðŸ“¡ Headers:`, authRes.headers);
              
              if (authRes.statusCode === 200) {
                const fileStream = fs.createWriteStream(outputPath);
                let audioData = Buffer.alloc(0);
                
                authRes.on('data', (chunk) => {
                  audioData = Buffer.concat([audioData, chunk]);
                });
                
                // Stop recording after 5 seconds
                setTimeout(() => {
                  authReq.destroy();
                  console.log('      â±ï¸  Recording stopped after 5 seconds');
                  
                  if (audioData.length > 0) {
                    fileStream.write(audioData);
                    fileStream.end();
                    console.log(`      âœ… Audio recording completed! (${audioData.length} bytes)`);
                    resolve(outputPath);
                  } else {
                    fileStream.end();
                    console.log('      âš ï¸  No audio data received');
                    reject(new Error('No audio data received'));
                  }
                }, 5000);
                
              } else {
                console.error('      âŒ Audio recording failed:', authRes.statusCode);
                reject(new Error(`Audio recording failed with status ${authRes.statusCode}`));
              }
            });
            
            authReq.on('error', (error) => {
              console.error('      âŒ Authenticated request error:', error.message);
              reject(error);
            });
            
            authReq.end();
            
          } else {
            console.error('      âŒ Could not parse digest challenge');
            reject(new Error('Could not parse digest challenge'));
          }
          
        } else {
          console.error('      âŒ Unexpected response:', res.statusCode);
          reject(new Error(`Unexpected response: ${res.statusCode}`));
        }
      });
      
      initialReq.on('error', (error) => {
        console.error('      âŒ Initial request error:', error.message);
        reject(error);
      });
      
      initialReq.end();
      
    } catch (error) {
      reject(error);
    }
  });
}

// Play audio through camera speaker using Amcrest postAudio API
async function playAudioThroughCamera(audioFile) {
  return new Promise((resolve, reject) => {
    // Add timeout to prevent hanging
    const timeout = setTimeout(() => {
      console.log('      â±ï¸  Audio playback timeout after 10 seconds');
      resolve(); // Don't reject, just resolve to continue
    }, 10000);
    try {
      const http = require('http');
      const crypto = require('crypto');
      
      // Read the audio file
      const audioData = fs.readFileSync(audioFile);
      console.log(`      ðŸ“ Audio file size: ${audioData.length} bytes`);
      
      // First request to get digest challenge
      const initialOptions = {
        hostname: cameraConfig.hostname,
        port: cameraConfig.port,
        path: '/cgi-bin/audio.cgi?action=postAudio&httptype=singlepart&channel=1',
        method: 'POST',
        headers: {
          'Content-Type': 'Audio/AAC',
          'Content-Length': audioData.length,
          'User-Agent': 'Amcrest-Camera-Client/1.0'
        }
      };
      
      console.log('      ðŸ“¡ Sending audio to camera speaker...');
      
      const initialReq = http.request(initialOptions, (res) => {
        console.log(`      ðŸ“¡ Initial HTTP Response: ${res.statusCode}`);
        
        if (res.statusCode === 401 && res.headers['www-authenticate']) {
          // Parse digest challenge
          const authHeader = res.headers['www-authenticate'];
          console.log(`      ðŸ” Digest challenge: ${authHeader}`);
          
          // Extract digest parameters
          const realm = authHeader.match(/realm="([^"]+)"/)?.[1];
          const nonce = authHeader.match(/nonce="([^"]+)"/)?.[1];
          const qop = authHeader.match(/qop="([^"]+)"/)?.[1];
          const opaque = authHeader.match(/opaque="([^"]+)"/)?.[1];
          
          if (realm && nonce) {
            // Generate digest response
            const cnonce = crypto.randomBytes(16).toString('hex');
            const nc = '00000001';
            const uri = initialOptions.path;
            const method = 'POST';
            
            // Calculate digest response
            const ha1 = crypto.createHash('md5').update(`${cameraConfig.username}:${realm}:${cameraConfig.password}`).digest('hex');
            const ha2 = crypto.createHash('md5').update(`${method}:${uri}`).digest('hex');
            
            let response;
            if (qop === 'auth') {
              response = crypto.createHash('md5').update(`${ha1}:${nonce}:${nc}:${cnonce}:${qop}:${ha2}`).digest('hex');
            } else {
              response = crypto.createHash('md5').update(`${ha1}:${nonce}:${ha2}`).digest('hex');
            }
            
            // Build digest authorization header
            let digestAuth = `Digest username="${cameraConfig.username}", realm="${realm}", nonce="${nonce}", uri="${uri}", response="${response}"`;
            if (opaque) digestAuth += `, opaque="${opaque}"`;
            if (qop) digestAuth += `, qop=${qop}, nc=${nc}, cnonce="${cnonce}"`;
            
            console.log(`      ðŸ” Digest auth: ${digestAuth}`);
            
            // Make authenticated request
            const authOptions = {
              hostname: cameraConfig.hostname,
              port: cameraConfig.port,
              path: '/cgi-bin/audio.cgi?action=postAudio&httptype=singlepart&channel=1',
              method: 'POST',
              headers: {
                'Content-Type': 'Audio/AAC',
                'Content-Length': audioData.length,
                'Authorization': digestAuth,
                'User-Agent': 'Amcrest-Camera-Client/1.0'
              }
            };
            
            const authReq = http.request(authOptions, (authRes) => {
              let responseData = '';
              
              authRes.on('data', (chunk) => {
                responseData += chunk.toString();
              });
              
              authRes.on('end', () => {
                console.log(`      ðŸ“¡ Authenticated HTTP Response: ${authRes.statusCode}`);
                console.log(`      ðŸ“¡ Response: ${responseData}`);
                
                if (authRes.statusCode === 200 && responseData.includes('OK')) {
                  console.log('      âœ… Audio sent to camera speaker successfully!');
                } else {
                  console.log('      âš ï¸  Audio playback may not be supported or failed');
                }
                clearTimeout(timeout);
                resolve();
              });
            });
            
            authReq.on('error', (err) => {
              console.log('      âŒ Authenticated request error:', err.message);
              console.log('      ðŸ’¡ Camera may not support audio playback via HTTP');
              clearTimeout(timeout);
              resolve(); // Don't reject, just note that it's not supported
            });
            
            authReq.write(audioData);
            authReq.end();
            
                  } else {
          console.error('      âŒ Could not parse digest challenge');
          clearTimeout(timeout);
          resolve(); // Don't reject, just note the issue
        }
          
        } else {
          console.log(`      ðŸ“¡ Unexpected response: ${res.statusCode}`);
          clearTimeout(timeout);
          resolve(); // Don't reject, just note the issue
        }
      });
      
      initialReq.on('error', (err) => {
        console.log('      âŒ Initial request error:', err.message);
        console.log('      ðŸ’¡ Camera may not support audio playback via HTTP');
        clearTimeout(timeout);
        resolve(); // Don't reject, just note that it's not supported
      });
      
      initialReq.write(audioData);
      initialReq.end();
      
    } catch (error) {
      console.log('      âŒ Audio playback error:', error.message);
      clearTimeout(timeout);
      resolve(); // Don't reject, just note the error
    }
  });
}

// Handle user question with AI vision (optimized for speed)
async function handleUserQuestion(question) {
  try {
    console.log('ðŸ¤– Processing question with AI vision...');
    
    // Capture current frame
    const rtspUrl = `rtsp://${cameraConfig.username}:${cameraConfig.password}@${cameraConfig.hostname}:${cameraConfig.port}/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif`;
    const imageBase64 = await captureFrame(rtspUrl);
    
    if (imageBase64) {
      // Send question and image to AI with optimized prompt
      const messages = [
        {
          role: 'system',
          content: AI_PROVIDER === 'chatgpt' 
            ? 'You are a camera assistant that answers yes/no questions about what you see. You MUST respond with ONLY "YES" or "NO" - no other text, no explanations, no punctuation. Just "YES" or "NO".'
            : 'You are a camera assistant that answers yes/no questions about what you see. Respond ONLY with "YES" or "NO" based on the image. Be direct and concise.'
        },
        {
          role: 'user',
          content: question,
          images: [imageBase64]
        }
      ];
      
      const answer = await aiChat(messages);
      if (answer) {
        const answerTrimmed = answer.trim();
        console.log('ðŸ¤– AI Answer:', answerTrimmed);
        
        // Determine gesture based on answer (case-insensitive)
        const answerLower = answerTrimmed.toLowerCase();
        if (answerLower === 'yes' || answerLower.includes('yes')) {
          console.log('âœ… Answer is YES - Camera will nod in response');
          gestureInProgress = true;
          gestureYes(() => {
            console.log('âœ… Camera nodded "YES" to your question');
            gestureInProgress = false;
          });
        } else if (answerLower === 'no' || answerLower.includes('no')) {
          console.log('âŒ Answer is NO - Camera will shake in response');
          gestureInProgress = true;
          gestureNo(() => {
            console.log('âœ… Camera shook "NO" to your question');
            gestureInProgress = false;
          });
        } else {
          console.log('ðŸ¤” Ambiguous answer - Camera will not gesture');
          console.log('ðŸ’¡ Try asking a yes/no question for a gesture response');
        }
      }
    }
  } catch (error) {
    console.error('âŒ Question processing error:', error.message);
  }
}

// Start voice interaction system
function startVoiceInteraction() {
  console.log('\nðŸŽ¤ Voice Interaction System Ready!');
  console.log('ðŸ¤– Camera is ready to answer yes/no questions with gestures...');
  console.log('ðŸ’¡ Press "V" for voice question, "T" for text question, "M" for mic test, "W" for wake word mode, "Q" to quit');
  
  // Set up keyboard listener
  process.stdin.setRawMode(true);
  process.stdin.resume();
  process.stdin.setEncoding('utf8');
  
  process.stdin.on('data', async (key) => {
    // Handle Ctrl+C (ASCII 3)
    if (key === '\u0003') {
      console.log('\nðŸ›‘ Ctrl+C detected - stopping application...');
      process.stdin.setRawMode(false);
      process.stdin.pause();
      stopVisionAnalysis();
      cam.stop(() => {
        console.log('ðŸ‘‹ Demo stopped. Goodbye!');
        process.exit(0);
      });
      return;
    }
    
    if (key === 'v' || key === 'V') {
      console.log('\nðŸŽ¤ Voice recording triggered! Speak your question now...');
      startVoiceRecording();
    } else if (key === 't' || key === 'T') {
      console.log('\nðŸ“ Text input mode - Type your question and press Enter:');
      process.stdin.setRawMode(false);
      process.stdin.setEncoding('utf8');
      
      process.stdin.once('data', async (question) => {
        const cleanQuestion = question.toString().trim();
        console.log('ðŸŽ¤ Text question:', cleanQuestion);
        await handleUserQuestion(cleanQuestion);
        
        // Return to raw mode for keyboard shortcuts
        process.stdin.setRawMode(true);
        process.stdin.setEncoding('utf8');
        console.log('\nðŸ’¡ Press "V" for voice, "T" for text, "M" for mic test, "Q" to quit');
      });
    } else if (key === 'm' || key === 'M') {
      console.log('\nðŸŽ¤ Starting microphone and speaker test...');
      await testMicrophoneAndSpeaker();
      console.log('\nðŸ’¡ Press "V" for voice, "T" for text, "M" for mic test, "W" for wake word mode, "Q" to quit');
    } else if (key === 'w' || key === 'W') {
      console.log('\nðŸ‘‚ Starting wake word detection mode...');
      console.log(`ðŸŽ¯ Say "${WAKE_WORD_CONFIG.wakePhrase}" to activate the camera!`);
      startWakeWordMode();
    } else if (key === 'q' || key === 'Q') {
      console.log('\nðŸ‘‹ Quitting...');
      process.stdin.setRawMode(false);
      process.stdin.pause();
      stopVisionAnalysis();
      process.exit(0);
    }
  });
}

// Wake word mode - continuously listen for wake word
async function startWakeWordMode() {
  console.log('ðŸ‘‚ Wake word mode activated!');
  console.log(`ðŸŽ¯ Continuously listening for: "${WAKE_WORD_CONFIG.wakePhrase}"`);
  console.log('ðŸ’¡ Press any key to exit wake word mode');
  
  // Start continuous wake word detection
  await monitorForWakeWord();
}

// Monitor for wake word continuously
async function monitorForWakeWord() {
  let wakeWordActive = true;
  
  while (wakeWordActive) {
    try {
      console.log('ðŸ‘‚ Listening for wake word...');
      
      // Record audio and check for wake word
      const audioFile = await recordAudioFromCamera();
      
      if (audioFile) {
        // Check if wake word is present
        const transcribedText = await convertSpeechToText(audioFile);
        
        if (transcribedText && transcribedText !== 'No speech detected') {
          const textLower = transcribedText.toLowerCase().trim();
          const wakePhraseLower = WAKE_WORD_CONFIG.wakePhrase.toLowerCase();
          
          console.log('ðŸ” Transcribed:', transcribedText);
          
          // Check for wake word variations (e.g., "jarvis", "jar vis", "journalist", "jar", "vis")
          const wakeWordDetected = textLower.includes(wakePhraseLower) || 
                                  textLower.includes('jar vis') ||
                                  textLower.includes('journalist') ||
                                  (textLower.includes('jar') && textLower.includes('vis'));
          
          if (wakeWordDetected) {
            console.log('ðŸŽ¯ Wake word detected! Processing question...');
            
            // Extract the question from the original audio (remove wake word)
            const questionStart = textLower.indexOf(wakePhraseLower) + wakePhraseLower.length;
            const questionText = transcribedText.substring(questionStart).trim();
            
            if (questionText && questionText !== '[ BLANK _ AUDIO ]') {
              console.log('ðŸŽ¤ Extracted question:', questionText);
              await handleUserQuestion(questionText);
            } else {
              console.log('âŒ No question detected after wake word');
            }
            
            console.log('ðŸ‘‚ Resuming wake word detection...');
          }
        }
        
        // Clean up audio file
        if (fs.existsSync(audioFile)) {
          fs.unlinkSync(audioFile);
        }
      }
      
      // Small delay before next check
      await new Promise(resolve => setTimeout(resolve, 1000));
      
    } catch (error) {
      console.error('âŒ Wake word monitoring error:', error.message);
      await new Promise(resolve => setTimeout(resolve, 2000));
    }
  }
}

// Stop AI vision analysis
function stopVisionAnalysis() {
  visionAnalysisActive = false;
  aiAnalysisInProgress = false;
  gestureInProgress = false;
  voiceRecordingActive = false;
  if (currentAudioStream) {
    currentAudioStream.stop();
    currentAudioStream = null;
  }
  console.log('â¹ï¸  AI Vision Analysis stopped');
}

const cam = new Cam(cameraConfig, async function(err) {
  if (err) {
    console.error('âŒ Failed to connect to camera:', err.message);
    process.exit(1);
  }
  
  console.log('âœ… Successfully connected to camera!');
  
  // Initialize Ollama
  const aiReady = await initializeAI();
  if (!aiReady) {
    console.log('âš ï¸  Continuing without AI vision capabilities...');
  }
  
  // Get camera information
  console.log('\nðŸ“· Camera Information:');
  console.log(`   Manufacturer: ${cam.hostname}`);
  console.log(`   Model: ${cam.name}`);
  console.log(`   Hardware ID: ${cam.hardwareId}`);
  console.log(`   Location: ${cam.location}`);
  
  // Get PTZ configuration
  cam.getConfigurations((err, configs) => {
    if (err) {
      console.error('âŒ Failed to get PTZ configurations:', err.message);
      return;
    }
    
    console.log('\nðŸŽ® PTZ Configurations found:', configs.length);
    
    // Get stream URI
    cam.getStreamUri({ protocol: 'RTSP' }, (err, res) => {
      if (!err) {
        console.log('\nðŸ“º RTSP Stream URL:', res.uri);
        
        // Add authentication to RTSP URL
        const authenticatedRtspUrl = res.uri.replace('rtsp://', `rtsp://${cameraConfig.username}:${cameraConfig.password}@`);
        console.log('ðŸ” Authenticated RTSP URL:', authenticatedRtspUrl);
        
        // Start AI vision analysis if Ollama is ready
        if (aiReady) {
          console.log('\nðŸš€ Starting AI Vision Analysis...');
          startVisionAnalysis(authenticatedRtspUrl);
        }
        
        // Start voice interaction system
        startVoiceInteraction();
        
        // Start PTZ demo
        // startPersonalityDemo();
      } else {
        console.error('âŒ Failed to get RTSP URL:', err.message);
        // startPersonalityDemo();
      }
    });
  });
});

// Check what audio capabilities the camera supports
function checkAudioCapabilities(callback) {
  console.log('\nðŸ”Š Checking Audio Capabilities...');
  
  let audioChecks = 0;
  const totalChecks = 3;
  
  function checkComplete() {
    audioChecks++;
    if (audioChecks >= totalChecks) {
      console.log('   âœ… Audio capability check completed');
      if (callback) callback();
    }
  }
  
  // Check for audio sources (microphones)
  cam.getAudioSources((err, sources) => {
    if (!err && sources && sources.length > 0) {
      console.log('   ðŸŽ¤ Audio Sources found:', sources.length);
      sources.forEach((source, index) => {
        console.log(`      ${index + 1}. ${source.name || 'Unknown'} (${source.token})`);
        if (source.configurations) {
          console.log(`         Configurations: ${source.configurations.length}`);
        }
      });
    } else {
      console.log('   âŒ No audio sources found or error:', err ? err.message : 'No sources');
    }
    checkComplete();
  });
  
  // Check for audio outputs (speakers)
  cam.getAudioOutputs((err, outputs) => {
    if (!err && outputs && outputs.length > 0) {
      console.log('   ðŸ”Š Audio Outputs found:', outputs.length);
      outputs.forEach((output, index) => {
        console.log(`      ${index + 1}. ${output.name || 'Unknown'} (${output.token})`);
        if (output.configurations) {
          console.log(`         Configurations: ${output.configurations.length}`);
        }
      });
    } else {
      console.log('   âŒ No audio outputs found or error:', err ? err.message : 'No outputs');
    }
    checkComplete();
  });
  
  // Check for audio encoder configurations
  cam.getAudioEncoderConfigurations((err, configs) => {
    if (!err && configs && configs.length > 0) {
      console.log('   ðŸŽµ Audio Encoder Configurations found:', configs.length);
      configs.forEach((config, index) => {
        console.log(`      ${index + 1}. ${config.name || 'Unknown'} (${config.token})`);
        if (config.encoding) {
          console.log(`         Encoding: ${config.encoding}`);
        }
        if (config.bitrate) {
          console.log(`         Bitrate: ${config.bitrate}`);
        }
        if (config.sampleRate) {
          console.log(`         Sample Rate: ${config.sampleRate}`);
        }
      });
    } else {
      console.log('   âŒ No audio encoder configurations found or error:', err ? err.message : 'No configs');
    }
    checkComplete();
  });
}


// Enhanced demo with personality gestures
function startPersonalityDemo() {
  console.log('\nðŸŽ­ Starting Personality Demo...');
  console.log('ðŸ¤– Watch the device show some personality!');
  
  const personalitySequence = [
    { name: 'Pan Right', x: 0.3, y: 0.0, zoom: 0.0 },
    { name: 'Pan Left', x: -0.3, y: 0.0, zoom: 0.0 },
    { name: 'Tilt Up', x: 0.0, y: 0.3, zoom: 0.0 },
    { name: 'Tilt Down', x: 0.0, y: -0.3, zoom: 0.0 },
    { name: 'Return to Center', x: 0.0, y: 0.0, zoom: 0.0 }
  ];
  
  let currentIndex = 0;
  
  function executeNextMovement() {
    if (currentIndex >= personalitySequence.length) {
      // After basic movements, do personality gestures
      console.log('\nðŸŽ­ Time for some personality!');
      
      // Do "yes" gesture
      gestureYes(() => {
        setTimeout(() => {
          // Do "no" gesture
          gestureNo(() => {
            console.log('\nâœ… Personality Demo completed!');
            console.log('ðŸ¤– AI Vision Analysis will continue running...');
            console.log('ðŸ’¡ Press Ctrl+C to stop the application');
          });
        }, 2000);
      });
      return;
    }
    
    const movement = personalitySequence[currentIndex];
    console.log(`\nðŸ”„ ${movement.name}...`);
    console.log(`   Pan: ${movement.x}, Tilt: ${movement.y}, Zoom: ${movement.zoom}`);
    
    cam.continuousMove(movement, (err) => {
      if (err) {
        console.error(`âŒ Failed to execute ${movement.name}:`, err.message);
      } else {
        console.log(`   âœ… ${movement.name} started`);
      }
    });
    
    // Stop movement after 2 seconds
    setTimeout(() => {
      cam.stop((err) => {
        if (err) {
          console.error(`âŒ Failed to stop ${movement.name}:`, err.message);
        } else {
          console.log(`   â¹ï¸  ${movement.name} stopped`);
        }
        
        // Wait 1 second before next movement
        setTimeout(() => {
          currentIndex++;
          executeNextMovement();
        }, 1000);
      });
    }, 2000);
  }
  
  // Start the personality demo sequence
  executeNextMovement();
}

// Gesture functions for personality
function gestureYes(callback) {
  console.log('\nðŸ™‚ Device says "YES" (nodding up and down)...');
  
  let nodCount = 0;
  const maxNods = 2;
  
  function performNod() {
    if (nodCount >= maxNods) {
      // Return to center
      cam.continuousMove({ x: 0.0, y: 0.0, zoom: 0.0 }, (err) => {
        if (err) {
          console.error('âŒ Failed to return to center:', err.message);
        } else {
          console.log('   âœ… Returned to center');
        }
        
        setTimeout(() => {
          cam.stop(() => {
            console.log('   ðŸŽ­ "Yes" gesture completed!');
            if (callback) callback();
          });
        }, 1000);
      });
      return;
    }
    
    console.log(`   Nod ${nodCount + 1}/${maxNods}: Tilting up...`);
    // Nod up - larger movement and longer duration
    cam.continuousMove({ x: 0.0, y: 0.3, zoom: 0.0 }, (err) => {
      if (err) {
        console.error('âŒ Failed to nod up:', err.message);
        return;
      }
      
      setTimeout(() => {
        console.log(`   Nod ${nodCount + 1}/${maxNods}: Tilting down...`);
        // Nod down - larger movement and longer duration
        cam.continuousMove({ x: 0.0, y: -0.3, zoom: 0.0 }, (err) => {
          if (err) {
            console.error('âŒ Failed to nod down:', err.message);
            return;
          }
          
          setTimeout(() => {
            nodCount++;
            performNod();
          }, 800);
        });
      }, 800);
    });
  }
  
  performNod();
}

function gestureNo(callback) {
  console.log('\nðŸ˜ Device says "NO" (shaking left and right)...');
  
  let shakeCount = 0;
  const maxShakes = 2;
  
  function performShake() {
    if (shakeCount >= maxShakes) {
      // Return to center
      cam.continuousMove({ x: 0.0, y: 0.0, zoom: 0.0 }, (err) => {
        if (err) {
          console.error('âŒ Failed to return to center:', err.message);
        } else {
          console.log('   âœ… Returned to center');
        }
        
        setTimeout(() => {
          cam.stop(() => {
            console.log('   ðŸŽ­ "No" gesture completed!');
            if (callback) callback();
          });
        }, 1000);
      });
      return;
    }
    
    console.log(`   Shake ${shakeCount + 1}/${maxShakes}: Panning left...`);
    // Shake left - larger movement and longer duration
    cam.continuousMove({ x: -0.3, y: 0.0, zoom: 0.0 }, (err) => {
      if (err) {
        console.error('âŒ Failed to shake left:', err.message);
        return;
      }
      
      setTimeout(() => {
        console.log(`   Shake ${shakeCount + 1}/${maxShakes}: Panning right...`);
        // Shake right - larger movement and longer duration
        cam.continuousMove({ x: 0.3, y: 0.0, zoom: 0.0 }, (err) => {
          if (err) {
            console.error('âŒ Failed to shake right:', err.message);
            return;
          }
          
          setTimeout(() => {
            shakeCount++;
            performShake();
          }, 800);
        });
      }, 800);
    });
  }
  
  performShake();
}

// Enhanced demo with personality gestures
function startPersonalityDemo() {
  console.log('\nðŸŽ­ Starting Personality Demo...');
  console.log('ðŸ¤– Watch the device show some personality!');
  
  const personalitySequence = [
    { name: 'Pan Right', x: 0.3, y: 0.0, zoom: 0.0 },
    { name: 'Pan Left', x: -0.3, y: 0.0, zoom: 0.0 },
    { name: 'Tilt Up', x: 0.0, y: 0.3, zoom: 0.0 },
    { name: 'Tilt Down', x: 0.0, y: -0.3, zoom: 0.0 },
    { name: 'Return to Center', x: 0.0, y: 0.0, zoom: 0.0 }
  ];
  
  let currentIndex = 0;
  
  function executeNextMovement() {
    if (currentIndex >= personalitySequence.length) {
      // After basic movements, do personality gestures
      console.log('\nðŸŽ­ Time for some personality!');
      
      // Do "yes" gesture
      gestureYes(() => {
        setTimeout(() => {
          // Do "no" gesture
          gestureNo(() => {
            console.log('\nâœ… Personality Demo completed!');
            console.log('ðŸ¤– AI Vision Analysis will continue running...');
            console.log('ðŸ’¡ Press Ctrl+C to stop the application');
          });
        }, 2000);
      });
      return;
    }
    
    const movement = personalitySequence[currentIndex];
    console.log(`\nðŸ”„ ${movement.name}...`);
    console.log(`   Pan: ${movement.x}, Tilt: ${movement.y}, Zoom: ${movement.zoom}`);
    
    cam.continuousMove(movement, (err) => {
      if (err) {
        console.error(`âŒ Failed to execute ${movement.name}:`, err.message);
      } else {
        console.log(`   âœ… ${movement.name} started`);
      }
    });
    
    // Stop movement after 2 seconds
    setTimeout(() => {
      cam.stop((err) => {
        if (err) {
          console.error(`âŒ Failed to stop ${movement.name}:`, err.message);
        } else {
          console.log(`   â¹ï¸  ${movement.name} stopped`);
        }
        
        // Wait 1 second before next movement
        setTimeout(() => {
          currentIndex++;
          executeNextMovement();
        }, 1000);
      });
    }, 2000);
  }
  
  // Start the personality demo sequence
  executeNextMovement();
}

// Handle process termination
process.on('SIGINT', () => {
  console.log('\nðŸ›‘ Stopping camera movements and AI vision...');
  stopVisionAnalysis();
  cam.stop(() => {
    console.log('ðŸ‘‹ Demo stopped. Goodbye!');
    process.exit(0);
  });
});

process.on('SIGTERM', () => {
  console.log('\nðŸ›‘ Stopping camera movements and AI vision...');
  stopVisionAnalysis();
  cam.stop(() => {
    console.log('ðŸ‘‹ Demo stopped. Goodbye!');
    process.exit(0);
  });
});



